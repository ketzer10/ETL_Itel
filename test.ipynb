{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misc functions scripts\n",
    "\n",
    "def print_log(message: str, function: str, where: str, type_message: str):\n",
    "    \"\"\"\n",
    "    Print a message with specific format\n",
    "\n",
    "    Args:\n",
    "        message (str): string that need be printed\n",
    "        function (str): string that indicates the origin function from the message\n",
    "        where (str): string that indicates the origin place/script from the message\n",
    "        type_message (str): _specify the type of message\n",
    "    \"\"\"\n",
    "    match type_message:\n",
    "        case \"log\":\n",
    "            print(f\"-- LOG -- {message} -- {function} -- {where}\" )\n",
    "        case \"error\":\n",
    "            print(f\"-- ERROR -- {message} -- {function} -- {where}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasbase component\n",
    "\n",
    "# import modules\n",
    "import utils.dfutils as df_tools\n",
    "import utils.dbutils as db_tools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# defining a place for log message\n",
    "where = \"load_dabase_components.py\"\n",
    "\n",
    "\n",
    "def load_save_insert_with_keys(df: pd.DataFrame, config: dict, **kargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Funtion designed for load data into dataframe. \n",
    "    In this case the data that has been previously saved in the data base table,\n",
    "    but that matches the data contained in the 'df' dataframe (taking into account\n",
    "    the columns that define the record as the only 'delete_keys') will be deleted\n",
    "    to replace it with the updated data coming from df, the rest of the data from\n",
    "    'df' will be added to the table in the database\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe with data that will be stored\n",
    "        config (dict): Dictionary with information that it's required for execute the function\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raised when occurring a error while execute this function\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with data that was stored\n",
    "        cross_component (dict): Dictionary with information that it's required for communicate components\n",
    "    \"\"\"\n",
    "    # defining a function for log message\n",
    "    function = \"load_save_insert_with_keys\"\n",
    "\n",
    "    # defining **kargs required as expliticty variables\n",
    "    between_dates = kargs[\"between_dates\"]\n",
    "    cross_parameters = kargs[\"cross_parameters\"]\n",
    "\n",
    "    try:\n",
    "        df = df_tools.fill_dataframe_nulls(df, \"\")\n",
    "        print_log(\"Opening connection to database\", function, where, \"log\")\n",
    "        conn = db_tools.open_connection_with_scripting_account()\n",
    "        cursor = conn.cursor() \n",
    "        cursor.fast_executemany = True\n",
    "        print_log(\"Loading data into database\", function, where, \"log\")\n",
    "        db_tools.perform_safe_delete_insert_with_keys(\n",
    "                conn = conn, \n",
    "                delete_keys = config[\"delete_keys\"], \n",
    "                source_df = df, \n",
    "                schema = config[\"schema\"], \n",
    "                target_table_name = config[\"table\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"ERROR: {e}\") \n",
    "    \n",
    "    return df, between_dates, cross_parameters \n",
    "\n",
    "\n",
    "def load_save_insert_with_keys_by_sheets(df: list, config: dict, **kargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Funtion designed for load data into dataframe. \n",
    "    In this case the data that has been previously saved in the data base table,\n",
    "    but that matches the data contained in the 'df' dataframe (taking into account\n",
    "    the columns that define the record as the only 'delete_keys') will be deleted\n",
    "    to replace it with the updated data coming from df, the rest of the data from\n",
    "    'df' will be added to the table in the database\n",
    "\n",
    "    Args:\n",
    "        df (list): List of Dataframe with data that will be stored\n",
    "        config (dict): Dictionary with information that it's required for execute the function\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raised when occurring a error while execute this function\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with data that was stored\n",
    "        cross_component (dict): Dictionary with information that it's required for communicate components\n",
    "    \"\"\"\n",
    "    # defining a function for log message\n",
    "    function = \"load_save_insert_with_keys_by_sheets\"\n",
    "\n",
    "    # defining **kargs required as expliticty variables\n",
    "    between_dates = kargs[\"between_dates\"]\n",
    "    cross_parameters = kargs[\"cross_parameters\"]\n",
    "    \n",
    "    for sheet in config.keys():\n",
    "        try:\n",
    "            df[sheet] = df_tools.fill_dataframe_nulls(df[sheet], \"\")\n",
    "            print_log(\"Opening connection to database\", function, where, \"log\")\n",
    "            conn = db_tools.open_connection_with_scripting_account()\n",
    "            cursor = conn.cursor() \n",
    "            cursor.fast_executemany = True\n",
    "            print_log(f\"Loading {sheet} data into database\", function, where, \"log\")\n",
    "            db_tools.perform_safe_delete_insert_with_keys(\n",
    "                    conn = conn, \n",
    "                    delete_keys = config[sheet][\"delete_keys\"], \n",
    "                    source_df = df[sheet], \n",
    "                    schema = config[sheet][\"schema\"], \n",
    "                    target_table_name = config[sheet][\"table\"]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"ERROR: {e}\")\n",
    "    \n",
    "    return df, between_dates, cross_parameters\n",
    "\n",
    "\n",
    "\n",
    "def download_data_from_database(df: pd.DataFrame, config: dict, **kargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads data from database and loads it into a Pandas dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe with data that will be stored\n",
    "        config (dict): Dictionary with information that it's required for execute the function\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raised when occurring a error while execute this function\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with data that was stored\n",
    "        cross_component (dict): Dictionary with information that it's required for communicate components\n",
    "    \"\"\"\n",
    "    # defining a function for log message\n",
    "    function = \"download_data_from_database\"\n",
    "\n",
    "    # defining **kargs required as expliticty variables\n",
    "    between_dates = kargs[\"between_dates\"]\n",
    "    cross_parameters = kargs[\"cross_parameters\"]\n",
    "\n",
    "    try:\n",
    "        print_log(\"Opening connection to database\", function, where, \"log\")\n",
    "        conn = db_tools.open_connection_with_scripting_account()\n",
    "        print_log(\"Downloading data from database\", function, where, \"log\")\n",
    "        df = db_tools.download_from_database(\n",
    "            conn = conn, \n",
    "            params =  config[\"params\"],\n",
    "            query = config[\"query\"]\n",
    "        )\n",
    "        df = df.dropna()\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"ERROR: {e}\") \n",
    "\n",
    "    return df, between_dates, cross_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline function scripts\n",
    "\n",
    "# import packages\n",
    "import pandas as pd\n",
    "\n",
    "# defining a place for log message\n",
    "where = \"pipeline.py\"\n",
    "\n",
    "def general_pipeline(config: dict, between_dates: dict):\n",
    "    # defining a function for log message\n",
    "    function = \"general_pipeline\"\n",
    "\n",
    "    dfs_df = pd.DataFrame\n",
    "    cross_parameters = dict()\n",
    "    for step in config.keys():\n",
    "        try:\n",
    "            print_log(f\"Executing {step}\", function, where, \"log\")\n",
    "            dfs_df, between_dates, cross_parameters = config[step][\"function\"](# function defined in config\n",
    "                # pass dataframe or dataframes list that will be manage\n",
    "                dfs_df,                                  \n",
    "                # pass dictionary defined in config with all information for execute the function \n",
    "                config[step][\"information_function\"], \n",
    "                # pass additional parameter, in this case each defined function should have **kargs \n",
    "                between_dates = between_dates,\n",
    "                cross_parameters = cross_parameters\n",
    "            )\n",
    "        except Exception as e:\n",
    "            message = f\"An error occurring while execute the step {step}. {e} \"\n",
    "            print_log(message, function,  where, \"error\")\n",
    "            raise Exception(message)\n",
    "            \n",
    "    return dfs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions scripts\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import os, time\n",
    "import utils.dfutils as df_tools\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "os.environ['TZ'] = 'GTM'\n",
    "time.tzset()\n",
    "\n",
    "# defining a place for log message\n",
    "where = \"observe_ia_functions.py\"\n",
    "\n",
    "def get_date_epoch_start():\n",
    "  reference = date(1970, 1, 1)#.date() \n",
    "#   dt = date.today() - timedelta(days = 1) # date.today() it's use because works for get the lastday\n",
    "\n",
    "# Revisar el dia 20\n",
    "  dt = date(2022,9,8) #- timedelta(days = 1)\n",
    "  return int((dt - reference).total_seconds() * 1000) #- 19*3600*1000 # This miliseconds ((19*3600 - 1)*1000) are subtracting for get a start time in day\n",
    "\n",
    "def get_date_epoch_end():\n",
    "  return get_date_epoch_start() + 24*3600*1000\n",
    "\n",
    "def api_component(dfs_df, information_function, **kargs):\n",
    "    # defining a function for log message\n",
    "    function = \"api_component\"\n",
    "    \n",
    "    # defining **kargs required as expliticty variables\n",
    "    between_dates = kargs[\"between_dates\"]\n",
    "    cross_parameters = kargs[\"cross_parameters\"]\n",
    "    \n",
    "    dfs = []\n",
    "    for account in information_function[\"accounts\"]:\n",
    "        try:\n",
    "            print_log(f\"Extracting data for account: {account}\", function, where, \"log\")\n",
    "            size = information_function[\"size\"] \n",
    "            page_i = information_function[\"page\"]\n",
    "            headers = information_function[\"headers\"]\n",
    "            payload = information_function[\"payload\"]\n",
    "\n",
    "            url = information_function[\"url_template\"].format(account = account, size = size, page = page_i)\n",
    "            response = json.loads(requests.request(\"POST\", url, headers = headers, data = payload).content)\n",
    "            print_log(f\"Extracting {response['totalPages']} pages for account: {account}\", function, where, \"log\")\n",
    "            df_accounts = []\n",
    "            for page in range(1, response[\"totalPages\"]+1):\n",
    "                try:\n",
    "                    url = information_function[\"url_template\"].format(account = account, size = size, page = page)\n",
    "                    response = json.loads(requests.request(\"POST\", url, headers = headers, data = payload).content)\n",
    "                    print_log(f\"Extracting page: {response['page']}\", function, where, \"log\")\n",
    "                    \n",
    "                    df_1 = pd.json_normalize(response, record_path = [\"meetings\"], meta = [\"account\", \"startDate\", \"endDate\"])\n",
    "                    df_1 = df_1[[\"observeCallId\", \"agentEmail\", \"agentName\", \"agentActive\", \"callTime\", \"scorecard\"]]\n",
    "                    df_2 = pd.json_normalize(response, record_path = [\"meetings\", [\"scorecard\"]], meta = [\"account\", \"startDate\"])\n",
    "                    df_2 = df_2[[\"account\", \"startDate\", \"present\", \"name\"]]\n",
    "                    df_2[\"comprobation_name\"] = None\n",
    "                    df_2[\"observeCallId\"] = None\n",
    "                    df_2[\"agentEmail\"] = None\n",
    "                    df_2[\"agentName\"] = None\n",
    "                    df_2[\"agentActive\"] = None\n",
    "                    df_2[\"callTime\"] = None\n",
    "                    \n",
    "                    df2_index = 0\n",
    "                    for df1_index in df_1.index:\n",
    "                        index_comprobation = 0\n",
    "                        for z in range(df2_index, df2_index + len(df_1.loc[df1_index,\"scorecard\"])):\n",
    "                            df_2.loc[z, \"comprobation_name\"] = df_1.loc[df1_index,\"scorecard\"][index_comprobation][\"name\"]\n",
    "                            df_2.loc[z, \"observeCallId\"] = df_1.loc[df1_index, \"observeCallId\"]\n",
    "                            df_2.loc[z, \"agentEmail\"] = df_1.loc[df1_index, \"agentEmail\"]\n",
    "                            df_2.loc[z, \"agentName\"] = df_1.loc[df1_index, \"agentName\"]\n",
    "                            df_2.loc[z, \"agentActive\"] = df_1.loc[df1_index, \"agentActive\"]\n",
    "                            df_2.loc[z, \"callTime\"] = df_1.loc[df1_index, \"callTime\"]\n",
    "                            index_comprobation = index_comprobation +1\n",
    "                            df2_index = df2_index + 1\n",
    "\n",
    "                    if (df_2[\"name\"] ==  df_2[\"comprobation_name\"]).unique() == True:\n",
    "                        df_2 = df_2[[\"account\", \"startDate\", \"observeCallId\", \"agentEmail\", \"agentName\", \"agentActive\", \"callTime\", \"present\", \"name\"]]\n",
    "                    else:\n",
    "                        print_log(f\"Error when trying extract page {page} for {account}. {e}\", function, where, \"error\")\n",
    "                        raise Exception(f\"Error when trying extract page {page} for {account}. {e}\")\n",
    "                    \n",
    "                    df_accounts.append(df_2)\n",
    "                except Exception as e:\n",
    "                    print_log(f\"Error when trying extract page {page} for {account}. {e}\", function, where, \"error\")\n",
    "                \n",
    "            df = pd.concat(df_accounts)\n",
    "            n = df[\"observeCallId\"].nunique() \n",
    "            df[\"startDate\"] = df[\"startDate\"].apply(lambda date: datetime.fromtimestamp(date/1000))\n",
    "            df = df.groupby(by=[\"account\", \"agentName\",\"name\",\"startDate\"]).sum(list(df['present'])).reset_index()\n",
    "            df[\"present\"] = ((df[\"present\"]/n))\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print_log(f\"Error when trying extract data for {account}. {e}\", function, where, \"error\")\n",
    "        \n",
    "    dfs_df = pd.concat(dfs, ignore_index = True)\n",
    "    return dfs_df, between_dates, cross_parameters\n",
    "\n",
    "def observeia_handling_function(df, information_function, **kargs):\n",
    "    # defining a function for log message\n",
    "    function = \"observeia_handling_function\"\n",
    "    \n",
    "    # defining **kargs required as expliticty variables\n",
    "    between_dates = kargs[\"between_dates\"]\n",
    "    cross_parameters = kargs[\"cross_parameters\"]\n",
    "\n",
    "    df = df_tools.df_handling(df, information_function[\"df_handling\"])\n",
    "    df.columns\n",
    "\n",
    "    return df, between_dates, cross_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"extract_step\": {\n",
    "        \"function\": api_component,\n",
    "        \"information_function\": {\n",
    "            \"accounts\": [\n",
    "                \"itelbpo_car8\",\n",
    "                \"itelbpo-breville-mono-prod\", \n",
    "                \"itelbpo-healthandwellness\", \n",
    "                \"itelbpo-speedyloan\", \n",
    "                \"itelbpo-thebradfordexchange\", \n",
    "                \"itelbpo_jps\", \n",
    "                \"itelbpo_jps_mono\", \n",
    "                \"itelbpo_ontellus\"\n",
    "            ],             \n",
    "            \"size\": 1000000,\n",
    "            \"page\": 1,\n",
    "            \"url_template\": \"https://api.observe.ai/v1/reports/accounts/{account}/data?size={size}&page={page}\",\n",
    "            \"payload\": json.dumps({\n",
    "              \"email\": \"reporting@itelinternational.com\",\n",
    "              \"start_date\": get_date_epoch_start(),\n",
    "              \"end_date\": get_date_epoch_end(),\n",
    "            }),\n",
    "            \"headers\": {\n",
    "              \"Content-Type\": \"application/json\",\n",
    "              \"Authorization\": \"Basic aXRlbGJwb19qcHM6UzphRj9aO1JVelh5SFs4cDtQQk8sVmFOTm8=\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"transform_step\": {\n",
    "        \"function\": observeia_handling_function,\n",
    "        \"information_function\": {\n",
    "          \"df_handling\": {\n",
    "            \"rename_columns\": { \n",
    "              \"account\": \"account\", \n",
    "              \"startDate\": \"date\", \n",
    "              \"agentName\": \"agent_name\", \n",
    "              \"present\": \"present_rate\", \n",
    "              \"name\": \"scorecard\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    },\n",
    "    \"load_step\": {\n",
    "      \"function\": load_save_insert_with_keys,\n",
    "      \"information_function\": {\n",
    "          \"delete_keys\": [\"date\", \"account\", \"scorecard\"], # required for load_database_components.load_save_insert_with_keys\n",
    "          \"schema\": \"learning_development\",          # required for load_database_components.load_save_insert_with_keys\n",
    "          \"table\": \"observe_ia_scorecards\"             # required for load_database_components.load_save_insert_with_keys\n",
    "      }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- LOG -- Executing extract_step -- general_pipeline -- observe_ia_functions.py\n",
      "-- LOG -- Extracting data for account: itelbpo_car8 -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting 0 pages for account: itelbpo_car8 -- api_component -- observe_ia_functions.py\n",
      "-- ERROR -- Error when trying extract data for itelbpo_car8. No objects to concatenate -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting data for account: itelbpo-breville-mono-prod -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting 1 pages for account: itelbpo-breville-mono-prod -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting page: 1 -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting data for account: itelbpo-healthandwellness -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting 0 pages for account: itelbpo-healthandwellness -- api_component -- observe_ia_functions.py\n",
      "-- ERROR -- Error when trying extract data for itelbpo-healthandwellness. No objects to concatenate -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting data for account: itelbpo-speedyloan -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting 0 pages for account: itelbpo-speedyloan -- api_component -- observe_ia_functions.py\n",
      "-- ERROR -- Error when trying extract data for itelbpo-speedyloan. No objects to concatenate -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting data for account: itelbpo-thebradfordexchange -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting 0 pages for account: itelbpo-thebradfordexchange -- api_component -- observe_ia_functions.py\n",
      "-- ERROR -- Error when trying extract data for itelbpo-thebradfordexchange. No objects to concatenate -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting data for account: itelbpo_jps -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting 0 pages for account: itelbpo_jps -- api_component -- observe_ia_functions.py\n",
      "-- ERROR -- Error when trying extract data for itelbpo_jps. No objects to concatenate -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting data for account: itelbpo_jps_mono -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting 4 pages for account: itelbpo_jps_mono -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting page: 1 -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting page: 2 -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting page: 3 -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting page: 4 -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting data for account: itelbpo_ontellus -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Extracting 0 pages for account: itelbpo_ontellus -- api_component -- observe_ia_functions.py\n",
      "-- ERROR -- Error when trying extract data for itelbpo_ontellus. No objects to concatenate -- api_component -- observe_ia_functions.py\n",
      "-- LOG -- Executing transform_step -- general_pipeline -- observe_ia_functions.py\n",
      "Rename columns\n",
      "-- LOG -- Executing load_step -- general_pipeline -- observe_ia_functions.py\n",
      "-- LOG -- Opening connection to database -- load_save_insert_with_keys -- observe_ia_functions.py\n",
      "-- LOG -- Loading data into database -- load_save_insert_with_keys -- observe_ia_functions.py\n",
      "Deleting data from database.\n",
      "Inserting data into database.\n",
      "Commiting changes.\n",
      "4094 rows inserted to the observe_ia_scorecards table\n",
      "Connection Closed\n"
     ]
    }
   ],
   "source": [
    "# entry point \n",
    "dfs_df = general_pipeline(configs, dict())\n",
    "# dfs_df.to_csv(\"observe_ia_all_accounts_05-25-2022.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d65aaef6228d26d9a64e4da73bbbf34cf769828bfbcbb1c2bad91e7440cf3de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
